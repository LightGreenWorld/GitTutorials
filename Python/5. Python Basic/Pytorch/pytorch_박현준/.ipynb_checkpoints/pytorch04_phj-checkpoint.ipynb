{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./.data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d51c97c292478caa974e6a60f7ec8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./.data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./.data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5381efa5c14b42b393c7d5f4258954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./.data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./.data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b598ab9b4d1b4ea0810c15bfcd39431a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./.data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./.data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11012a702efb47c98d9c9b64f0659517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./.data\\MNIST\\raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./.data', \n",
    "                                                          train = True, \n",
    "                                                          download = True,\n",
    "                                                          transform=transforms.Compose([transforms.ToTensor(), \n",
    "                                                                                        transforms.Normalize((0.1307,),(0.3081,))])),\n",
    "                                           batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('./.data',\n",
    "                                                        train=False,\n",
    "                                                        transform=transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Normalize((0.1307,),(0.3081,))\n",
    "                                                        ])),\n",
    "                                         batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN신경망 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss:{:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. *batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss:2.286855\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss:0.888106\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss:0.935208\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss:0.664370\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss:0.414379\n",
      "[1] Test Loss: 0.2014, Accuracy: 94.17%\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss:0.598893\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss:0.430799\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss:0.563276\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss:0.259087\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss:0.344607\n",
      "[2] Test Loss: 0.1248, Accuracy: 96.19%\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss:0.337133\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss:0.322542\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss:0.311093\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss:0.480636\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss:0.515567\n",
      "[3] Test Loss: 0.1014, Accuracy: 96.87%\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss:0.361688\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss:0.170743\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss:0.165467\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss:0.383194\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss:0.330885\n",
      "[4] Test Loss: 0.0870, Accuracy: 97.31%\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss:0.247376\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss:0.250689\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss:0.266919\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss:0.220807\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss:0.143036\n",
      "[5] Test Loss: 0.0764, Accuracy: 97.56%\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss:0.186535\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss:0.354485\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss:0.217358\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss:0.207346\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss:0.094038\n",
      "[6] Test Loss: 0.0717, Accuracy: 97.74%\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss:0.261118\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss:0.148015\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss:0.260841\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss:0.185003\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss:0.129167\n",
      "[7] Test Loss: 0.0644, Accuracy: 97.95%\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss:0.170506\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss:0.190997\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss:0.297099\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss:0.204006\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss:0.220851\n",
      "[8] Test Loss: 0.0626, Accuracy: 97.98%\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss:0.146403\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss:0.274308\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss:0.370970\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss:0.225571\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss:0.270576\n",
      "[9] Test Loss: 0.0617, Accuracy: 97.99%\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss:0.197676\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss:0.198165\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss:0.160646\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss:0.362743\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss:0.436306\n",
      "[10] Test Loss: 0.0607, Accuracy: 98.03%\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss:0.142232\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss:0.317597\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss:0.125332\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss:0.077315\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss:0.044873\n",
      "[11] Test Loss: 0.0541, Accuracy: 98.35%\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss:0.133727\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss:0.190882\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss:0.241488\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss:0.121376\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss:0.154326\n",
      "[12] Test Loss: 0.0519, Accuracy: 98.38%\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss:0.344620\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss:0.182933\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss:0.142109\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss:0.240258\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss:0.244463\n",
      "[13] Test Loss: 0.0507, Accuracy: 98.44%\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss:0.327735\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss:0.111167\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss:0.089819\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss:0.340470\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss:0.101359\n",
      "[14] Test Loss: 0.0489, Accuracy: 98.36%\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss:0.143660\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss:0.082199\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss:0.134963\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss:0.072003\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss:0.153956\n",
      "[15] Test Loss: 0.0464, Accuracy: 98.46%\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss:0.111942\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss:0.077696\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss:0.319371\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss:0.064565\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss:0.180051\n",
      "[16] Test Loss: 0.0449, Accuracy: 98.50%\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss:0.112181\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss:0.148050\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss:0.220564\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss:0.365825\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss:0.137741\n",
      "[17] Test Loss: 0.0470, Accuracy: 98.52%\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss:0.115066\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss:0.132308\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss:0.148610\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss:0.030941\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss:0.038175\n",
      "[18] Test Loss: 0.0461, Accuracy: 98.49%\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss:0.048881\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss:0.113859\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss:0.048447\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss:0.069213\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss:0.177523\n",
      "[19] Test Loss: 0.0462, Accuracy: 98.53%\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss:0.178937\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss:0.033078\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss:0.234696\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss:0.010415\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss:0.278207\n",
      "[20] Test Loss: 0.0434, Accuracy: 98.61%\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss:0.234244\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss:0.175302\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss:0.186744\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss:0.094762\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss:0.171866\n",
      "[21] Test Loss: 0.0416, Accuracy: 98.69%\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss:0.092808\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss:0.048104\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss:0.144193\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss:0.210285\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss:0.105602\n",
      "[22] Test Loss: 0.0446, Accuracy: 98.59%\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss:0.050809\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss:0.243992\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss:0.213604\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss:0.325891\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss:0.049882\n",
      "[23] Test Loss: 0.0415, Accuracy: 98.69%\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss:0.053467\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss:0.093139\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss:0.280639\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss:0.318943\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss:0.246687\n",
      "[24] Test Loss: 0.0393, Accuracy: 98.67%\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss:0.210779\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss:0.155164\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss:0.292270\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss:0.276231\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss:0.179223\n",
      "[25] Test Loss: 0.0400, Accuracy: 98.75%\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss:0.031366\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss:0.178818\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss:0.277486\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss:0.061870\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss:0.271992\n",
      "[26] Test Loss: 0.0405, Accuracy: 98.70%\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss:0.063179\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss:0.122125\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss:0.438733\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss:0.146196\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss:0.166376\n",
      "[27] Test Loss: 0.0394, Accuracy: 98.81%\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss:0.138442\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss:0.045463\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss:0.140350\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss:0.067985\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss:0.179940\n",
      "[28] Test Loss: 0.0383, Accuracy: 98.79%\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss:0.030675\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss:0.068627\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss:0.139543\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss:0.046176\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss:0.100559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29] Test Loss: 0.0369, Accuracy: 98.82%\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss:0.306937\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss:0.072390\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss:0.200718\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss:0.093992\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss:0.041430\n",
      "[30] Test Loss: 0.0377, Accuracy: 98.83%\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss:0.138869\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "    epoch, test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
