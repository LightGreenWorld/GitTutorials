{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 크롤링 기본\n",
    "\n",
    "* 크롤링 사이트: https://www.tripadvisor.co.kr/Restaurants-g294197-Seoul.html\n",
    "* robots.txt:  https://www.tripadvisor.co.kr/robots.txt\n",
    "\n",
    "\n",
    "### 크롤링 절차\n",
    "\n",
    "1. 사이트의 html을 읽어들이기: requests.get(url) 사용\n",
    "    \n",
    "2. 텍스트 형태의 데이터를 html 태그별로 구분하여 파싱하기 : BeutifulSoup\n",
    "\n",
    "3. 특정 태그값만 찾기 : findAll, find\n",
    "\n",
    "4. 필요한 데이터값 정제하기\n",
    "\n",
    "5. 데이터 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링할 웹 사이트 정보\n",
    "url = 'https://www.tripadvisor.co.kr/Restaurants-g294197-Seoul.html'\n",
    "# base_url = 'https://www.tripadvisor.co.kr/'\n",
    "# seoul_url = '/Restaurants-g294197-Seoul.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get html\n",
    "response = requests.get(url)\n",
    "# response = requests.get(base_url+seoul_url) \n",
    "\n",
    "# response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup을 활용하여 데이터 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# soup\n",
    "\n",
    "# <div class=\"wQjYiB7z\"><span><a href=\"/Restaurant_Review-g294197-d17423735-Reviews-Jangseng_Geongangwon-Seoul.html\" class=\"_15_ydu6b\" target=\"_blank\">1<!-- -->. <!-- -->장생건강원</a></span></div>\n",
    "lists = soup.findAll('div', {\"class\" : \"wQjYiB7z\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', ' 장생건강원']\n",
      "['2', ' 플레이버즈']\n",
      "['3', ' 구스토 타코']\n",
      "['4', ' 지화자']\n",
      "['5', ' 853']\n",
      "['6', ' 헴라갓']\n",
      "['7', ' 더그리핀바']\n",
      "['8', ' 최고집 홍대점']\n",
      "['9', ' 양국']\n",
      "['10', ' 정식당']\n",
      "['11', ' 브라이 리퍼블릭']\n",
      "['12', ' 타볼로 24']\n",
      "['13', ' 무교동 북어국집']\n",
      "['14', ' 교촌치킨 동대문1호점']\n",
      "['15', ' 광화문이층집']\n",
      "['16', ' 죠티인도레스토랑']\n",
      "['17', ' 브루클린 더버거조인트']\n",
      "['18', ' 더 파크뷰']\n",
      "['19', ' 유닭스토리']\n",
      "['20', ' 육전식당 - 본점']\n",
      "['21', ' 해도식당']\n",
      "['22', ' 쟈니덤플링']\n",
      "['23', ' 홍대 닭갈비']\n",
      "['24', ' 연남서식당']\n",
      "['25', ' 라이너스 바베큐']\n",
      "['26', ' 브레라']\n",
      "['27', ' 뉴델리']\n",
      "['28', ' 모모카페 코트야드 서울 남대문']\n",
      "['29', ' 라세느']\n",
      "['30', ' 자매집']\n"
     ]
    }
   ],
   "source": [
    "for item in lists: \n",
    "    name = item.text.split('.')\n",
    "    if len(name) == 2 :\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-76aba1e57488>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mhrefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mhref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhrefs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# 순위와 이름 분리하기, 광고 제거하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_url' is not defined"
     ]
    }
   ],
   "source": [
    "# 맛집 목록 가져오기\n",
    "lists = soup.findAll('div', {'class' : 'wQjYiB7z'}) \n",
    "\n",
    "# 데이터 파싱하기\n",
    "rest_id_list = []\n",
    "rest_name_list = []\n",
    "rest_url_list = []\n",
    "\n",
    "for item in lists:\n",
    "    # 링크 가져오기\n",
    "    hrefs = item.find('a')\n",
    "    href = hrefs.get('href')\n",
    "    full_url = base_url + href\n",
    "\n",
    "    # 순위와 이름 분리하기, 광고 제거하기\n",
    "    name = item.text.split('.')\n",
    "    if len(name) == 2:\n",
    "        rest_id = name[0]\n",
    "        rest_name = name[1]\n",
    "        rest_url = full_url\n",
    "        rest_id_list.append(rest_id)\n",
    "        rest_name_list.append(rest_name)\n",
    "        rest_url_list.append(rest_url)\n",
    "#         print('[ID]' + rest_id + ' [NAME]' + rest_name + ' [URL]' + rest_url)\n",
    "        \n",
    "result = pd.DataFrame(data={'[ID]' : rest_id_list, '[NAME]' : rest_name_list, '[URL]' : rest_url_list})\n",
    "# print(result)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (실습1) 추천글\n",
    "\n",
    "\n",
    "# <span class=\"_1OfeygW_\">“일딴 자리간격이 넓찍해서 코로나 걱정이 덜”</span>\n",
    "lists = soup.findAll('span', {\"class\" : \"_1OfeygW_\"})\n",
    "# print(lists)\n",
    "\n",
    "for item in lists: \n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (실습1-1) 리뷰 수\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <span class=\"w726Ki5B\">174건의 리뷰</span>\n",
    "lists = soup.findAll('span', {\"class\" : \"w726Ki5B\"})\n",
    "# print(lists)\n",
    "\n",
    "# for item in lists: \n",
    "#     numbers = re.findall(\"\\d+\", item.text)\n",
    "#     print(numbers)\n",
    "\n",
    "# for item in lists:\n",
    "#     url_temp = item.find('a')\n",
    "#     print(url_temp.get('herf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (실습2) 크롤링할 웹 사이트 정보\n",
    "# 'https://www.tripadvisor.co.kr/Attractions-g294197-Activities-c26-t142-Seoul.html#ATTRACTION_SORT_WRAPPER'\n",
    "\n",
    "# 서울소재 벼룩시장 정보 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_image = soup.findAll('div'. {class : })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (실습3) 이미지\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# 크롤링할 웹 사이트 정보\n",
    "base_url = 'https://www.tripadvisor.co.kr'\n",
    "seoul_url = '/Restaurants-g294197-Seoul.html'\n",
    "\n",
    "# get html\n",
    "response = requests.get(base_url + seoul_url)\n",
    "\n",
    "# BeautifulSoup을 활용하여 데이터 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 맛집 목록 가져오기\n",
    "lists = soup.findAll('div', {'class': 'wQjYiB7z'})\n",
    "hrefs = soup.find('a')\n",
    "# 데이터 파싱하기\n",
    "\n",
    "rest_id_list = []\n",
    "rest_name_list = []\n",
    "rest_url_list = []\n",
    "\n",
    "for item in lists:\n",
    "    # 링크 가져오기\n",
    "    href = hrefs.get('href')\n",
    "    full_url = base_url + href\n",
    "\n",
    "    # 순위와 이름 분리하기, 광고 제거하기\n",
    "    name = item.text.split('.')\n",
    "    if len(name) == 2:\n",
    "        rest_id = name[0]\n",
    "        rest_name = name[1]\n",
    "        rest_url = full_url\n",
    "        rest_id_list.append(rest_id)\n",
    "        rest_name_list.append(rest_name)\n",
    "        rest_url_list.append(rest_url)\n",
    "        # print('[ID]' + rest_id + ' [NAME]' + rest_name + ' [URL]' + rest_url)\n",
    "result = pd.DataFrame(data={'[ID]' : rest_id_list, '[NAME]' : rest_name_list, '[URL]' : rest_url_list})\n",
    "print(result)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "164px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
